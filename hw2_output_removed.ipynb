{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import pprint\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import os, tqdm\n",
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def unpickle_from_file(file_name):\n",
    "    with open(file_name, 'rb') as handle:\n",
    "        return pkl.load(handle)\n",
    "\n",
    "random.seed(134)\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "idx_to_label = ['contradiction', 'entailment', 'neutral']\n",
    "label_to_idx = {\n",
    "    'contradiction': 0,\n",
    "    'entailment': 1,\n",
    "    'neutral': 2\n",
    "}\n",
    "    \n",
    "def token2index_dataset(tokens_data, token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens.split(\" \")]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "def label2index_dataset(labels_data, label2id):\n",
    "    indices_data = []\n",
    "    for label in labels_data:\n",
    "        indices_data.append(label2id[label])\n",
    "    return indices_data\n",
    "\n",
    "def plot_train_validation(loss_lists, val_acc_lists, param, string_xticks=False):\n",
    "    figure = plt.figure(figsize = (20, 5))\n",
    "    figure.subplots_adjust(wspace = 0.05, hspace = 0.05)\n",
    "\n",
    "    figure.add_subplot(1,3,1)\n",
    "    for key, loss_list in loss_lists.items():\n",
    "        plt.plot(list(range(len(loss_list))), loss_list, alpha=0.5, label=str(key))\n",
    "    plt.legend()\n",
    "    plt.title(\"{}, training\".format(param))\n",
    "    \n",
    "    figure.add_subplot(1,3,2)\n",
    "    val_max = 0\n",
    "    val_max_key = 0\n",
    "    val_key_list = []\n",
    "    val_max_list = []\n",
    "    for key, val_acc_list in val_acc_lists.items():\n",
    "        plt.plot(list(range(len(val_acc_list))), val_acc_list, alpha=0.5, label=str(key))\n",
    "        current_max = max(val_acc_list)\n",
    "        val_key_list.append(key)\n",
    "        val_max_list.append(current_max)\n",
    "        if current_max > val_max:\n",
    "            val_max = current_max\n",
    "            val_max_key = key\n",
    "            \n",
    "    plt.legend()\n",
    "    plt.title(\"{}, validation\".format(param))\n",
    "    \n",
    "    figure.add_subplot(1,3,3)\n",
    "    if string_xticks:\n",
    "        x = list(range(len(val_key_list)))\n",
    "        my_xticks = val_key_list\n",
    "        plt.xticks(x, my_xticks)\n",
    "        plt.plot(x, val_max_list)\n",
    "    else:\n",
    "        plt.plot(val_key_list, val_max_list)\n",
    "    plt.title(\"best validation acc for each {}\".format(param))\n",
    "    print(val_key_list, val_max_list)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return val_max_key, val_max\n",
    "\n",
    "def plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, param, string_xticks=False, log=False, bar=False):\n",
    "    figure = plt.figure(figsize = (15, 10))\n",
    "    figure.subplots_adjust(wspace = 0.05, hspace = 0.05)\n",
    "\n",
    "    figure.add_subplot(2,2,1)\n",
    "    for key, loss_list in loss_lists.items():\n",
    "        plt.plot(list(range(len(loss_list))), loss_list, alpha=0.5, label=str(key))\n",
    "    plt.legend()\n",
    "    plt.title(\"{}, training loss\".format(param))\n",
    "    \n",
    "    figure.add_subplot(2,2,2)\n",
    "    for key, loss_list in val_loss_lists.items():\n",
    "        plt.plot(list(range(len(loss_list))), loss_list, alpha=0.5, label=str(key))\n",
    "    plt.legend()\n",
    "    plt.title(\"{}, validation loss\".format(param))\n",
    "    figure.add_subplot(2,2,3) \n",
    "    val_max = 0\n",
    "    val_max_key = 0\n",
    "    val_key_list = []\n",
    "    val_max_list = []\n",
    "    for key, val_acc_list in val_acc_lists.items():\n",
    "        plt.plot(list(range(len(val_acc_list))), val_acc_list, alpha=0.5, label=str(key))\n",
    "        current_max = max(val_acc_list)\n",
    "        val_key_list.append(key)\n",
    "        val_max_list.append(current_max)\n",
    "        if current_max > val_max:\n",
    "            val_max = current_max\n",
    "            val_max_key = key\n",
    "            \n",
    "    plt.legend()\n",
    "    plt.title(\"{}, validation acc\".format(param))\n",
    "     \n",
    "    \n",
    "    figure.add_subplot(2,2,4)\n",
    "    if string_xticks:\n",
    "        x = list(range(len(val_key_list)))\n",
    "        my_xticks = val_key_list\n",
    "        plt.xticks(x, my_xticks)\n",
    "        plt.plot(x, val_max_list)\n",
    "    else:\n",
    "        if bar:\n",
    "            plt.bar(val_key_list, val_max_list)\n",
    "        elif log:\n",
    "            plt.semilogx(val_key_list,val_max_list)\n",
    "        else:\n",
    "            plt.plot(val_key_list, val_max_list)\n",
    "    plt.title(\"best validation acc for each {}\".format(param))\n",
    "    print(val_key_list, val_max_list)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return val_max_key, val_max\n",
    "\n",
    "\n",
    "def set_words_data(words_to_load, ft_home = './'):\n",
    "    with open(ft_home + 'wiki-news-300d-1M.vec') as f:\n",
    "        loaded_embeddings_ft = np.random.random((words_to_load, 300)) # check to take care of pad and eos token\n",
    "        words_ft = {}\n",
    "        idx2words_ft = {}\n",
    "        ordered_words_ft = []\n",
    "\n",
    "        words_ft['<pad>'] = 0\n",
    "        idx2words_ft[0] = '<pad>' \n",
    "        words_ft['<unk>'] = 1\n",
    "        idx2words_ft[1] = '<unk>' \n",
    "\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines[1:]):\n",
    "            if i+2 >= words_to_load: \n",
    "                break\n",
    "            s = line.split()\n",
    "            # don't skip just because punctuation, too much complication\n",
    "            loaded_embeddings_ft[i+2, :] = np.asarray(s[1:])\n",
    "            words_ft[s[0]] = i+2\n",
    "            idx2words_ft[i+2] = s[0]\n",
    "            ordered_words_ft.append(s[0])\n",
    "    return words_ft, idx2words_ft, ordered_words_ft, loaded_embeddings_ft\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Test with one RNN with shared weights, rather than using separate two\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, loaded_embeddings_ft, emb_size, hidden_size, num_layers, num_classes, vocab_size, shuffle=False, interaction='concat', relu=False):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(RNN, self).__init__()\n",
    "        self.relu = relu\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        if loaded_embeddings_ft is not None:\n",
    "            self.embedding.load_state_dict({'weight': torch.cuda.FloatTensor(loaded_embeddings_ft)})\n",
    "        \n",
    "        self.shuffle=shuffle\n",
    "        \n",
    "        self.rnn1 = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True) \n",
    "        self.rnn2 = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.interaction = interaction\n",
    "        if interaction == 'concat':\n",
    "            self.linear1 = nn.Linear(hidden_size*2*2, hidden_size)\n",
    "        elif interaction == 'featurewise_multiplication':\n",
    "            self.linear1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers*2, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden.cuda()\n",
    "\n",
    "    def forward(self, x1, x2, lengths1, length2, reorder_sent_2):\n",
    "        # reset hidden state\n",
    "\n",
    "        batch_size, seq_len1 = x1.size()\n",
    "        batch_size, seq_len2 = x2.size()\n",
    "\n",
    "        self.hidden1 = self.init_hidden(batch_size)\n",
    "        self.hidden2 = self.init_hidden(batch_size)\n",
    "        \n",
    "        embed1 = self.embedding(x1)\n",
    "        embed1 = torch.nn.utils.rnn.pack_padded_sequence(embed1, lengths1.cpu().numpy(), batch_first=True)\n",
    "        rnn_out1, self.hidden1 = self.rnn1(embed1, self.hidden1)\n",
    "        \n",
    "        embed2 = self.embedding(x2)\n",
    "        embed2 = torch.nn.utils.rnn.pack_padded_sequence(embed2, length2.cpu().numpy(), batch_first=True)\n",
    "        rnn_out2, self.hidden2 = self.rnn2(embed2, self.hidden2)\n",
    "        \n",
    "        # reorder to match batches between two rnn outputs\n",
    "        if not self.shuffle:\n",
    "            self.hidden2 = self.hidden2[:,reorder_sent_2,:] \n",
    "        \n",
    "        if self.interaction == 'concat':\n",
    "            combined_representation = torch.cat([torch.cat([self.hidden1[0],self.hidden1[1]],1), \n",
    "                                torch.cat([self.hidden2[0],self.hidden2[1]],1)],\n",
    "                               1)\n",
    "        elif self.interaction == 'featurewise_multiplication':\n",
    "            combined_representation = torch.mul(torch.cat([self.hidden1[0],self.hidden1[1]],1), \n",
    "                                                torch.cat([self.hidden2[0],self.hidden2[1]],1))\n",
    "\n",
    "        combined_representation = self.linear1(combined_representation)\n",
    "        if self.relu:\n",
    "            combined_representation = F.relu(combined_representation)\n",
    "        logits = self.linear2(combined_representation)\n",
    "        return logits\n",
    "\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, loaded_embeddings_ft, emb_size, hidden_size, kernel_size, num_layers, num_classes, vocab_size, shuffle=False, interaction='concat', relu=False):\n",
    "        # num_layers parameter is not used and ignored.\n",
    "        assert kernel_size % 2 == 1\n",
    "        padding_size = kernel_size // 2\n",
    "        super(CNN, self).__init__()\n",
    "        self.relu = relu\n",
    "        self.num_layers, self.hidden_size, self.kernel_size = num_layers, hidden_size, kernel_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        if loaded_embeddings_ft is not None:\n",
    "            self.embedding.load_state_dict({'weight': torch.cuda.FloatTensor(loaded_embeddings_ft)})\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=kernel_size, padding=padding_size)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=kernel_size, padding=padding_size)\n",
    "\n",
    "        #self.linear1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.interaction = interaction \n",
    "        \n",
    "        if interaction == 'concat':\n",
    "            self.linear1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        elif interaction == 'featurewise_multiplication':\n",
    "            self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        self.shuffle=shuffle\n",
    "    def forward(self, x1, x2, lengths1, length2, reorder_sent_2):\n",
    "        # reset hidden state\n",
    "\n",
    "        batch_size, seq_len1 = x1.size()\n",
    "        batch_size, seq_len2 = x2.size()\n",
    "        \n",
    "        # get embedding of words\n",
    "        embed1 = self.embedding(x1)\n",
    "        \n",
    "        hidden1 = self.conv1(embed1.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size, seq_len1, hidden1.size(-1))\n",
    "\n",
    "        hidden1 = self.conv2(hidden1.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size, seq_len1, hidden1.size(-1))\n",
    "                \n",
    "        # get embedding of words\n",
    "        embed2 = self.embedding(x2)\n",
    "        \n",
    "        \n",
    "        hidden2 = self.conv1(embed2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, seq_len2, hidden2.size(-1))\n",
    "\n",
    "        hidden2 = self.conv2(hidden2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, seq_len1, hidden2.size(-1))\n",
    "        \n",
    "        # reorder to match batches between two rnn outputs\n",
    "        if not self.shuffle:\n",
    "            hidden2 = hidden2[reorder_sent_2,:,:] \n",
    "        \n",
    "        hidden1 = F.max_pool1d(hidden1.transpose(1,2), hidden1.size(1)).transpose(1,2)\n",
    "        hidden2 = F.max_pool1d(hidden2.transpose(1,2), hidden2.size(1)).transpose(1,2)\n",
    "        \n",
    "        #print('hidden2 size', hidden2.shape)\n",
    "        if self.interaction == 'concat':\n",
    "            combined_representation = torch.cat([hidden1, hidden2], 2)\n",
    "        elif self.interaction == 'featurewise_multiplication':\n",
    "            combined_representation = torch.mul(hidden1, hidden2)\n",
    "\n",
    "        combined_representation = self.linear1(combined_representation)\n",
    "        if self.relu:\n",
    "            combined_representation = F.relu(combined_representation)\n",
    "        logits = self.linear2(combined_representation)\n",
    "        return logits.squeeze(1)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mnli_train.tsv  mnli_val.tsv  snli_train.tsv  snli_val.tsv\n",
    "snil_train = pd.read_csv('snli_train.tsv', delimiter='\\t')\n",
    "snil_val = pd.read_csv('snli_val.tsv', delimiter='\\t')\n",
    "mnil_train = pd.read_csv('mnli_train.tsv', delimiter='\\t')\n",
    "mnil_val = pd.read_csv('mnli_val.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "class SNILDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sent1_list, sent2_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.sent1_list = sent1_list\n",
    "        self.sent2_list = sent2_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.sent1_list) == len(self.target_list)) and (len(self.sent2_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        sent1_token_idx = self.sent1_list[key][:MAX_SENTENCE_LENGTH] \n",
    "        sent2_token_idx = self.sent2_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [sent1_token_idx, sent2_token_idx, len(sent1_token_idx), len(sent2_token_idx), label, key]\n",
    "\n",
    "def snil_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    sent1_data_list = []\n",
    "    sent2_data_list = []\n",
    "    label_list = []\n",
    "    sent2_length_list = []\n",
    "    sent1_length_list = []\n",
    "    indices_list = []\n",
    "    for datum in batch:\n",
    "        indices_list.append(datum[5])\n",
    "        label_list.append(datum[4])\n",
    "        sent2_length_list.append(datum[3])\n",
    "        sent1_length_list.append(datum[2])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        sent1_padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        \n",
    "        sent2_padded_vec = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)   # TODO: Is it always returning length 200?\n",
    "        sent1_data_list.append(sent1_padded_vec)\n",
    "        sent2_data_list.append(sent2_padded_vec)\n",
    "        \n",
    "    ind_dec_order = np.argsort(sent1_length_list)[::-1]\n",
    "    sent1_data_list = np.array(sent1_data_list)[ind_dec_order]\n",
    "    sent1_length_list = np.array(sent1_length_list)[ind_dec_order]\n",
    "    sent2_data_list = np.array(sent2_data_list)[ind_dec_order]\n",
    "    sent2_length_list = np.array(sent2_length_list)[ind_dec_order]\n",
    "    label_list = np.array(label_list)[ind_dec_order]\n",
    "    indices_list = np.array(indices_list)[ind_dec_order]\n",
    "    \n",
    "    ind_dec_order_sent2 = np.argsort(sent2_length_list)[::-1]\n",
    "    sent2_data_list = np.array(sent2_data_list)[ind_dec_order_sent2]\n",
    "    sent2_length_list = np.array(sent2_length_list)[ind_dec_order_sent2]\n",
    "    \n",
    "    reorder_sent_2_dict = dict()\n",
    "    for i, each in enumerate(ind_dec_order_sent2):\n",
    "        reorder_sent_2_dict[each] = i\n",
    "    reorder_sent_2_list = []\n",
    "    for key, value in sorted(reorder_sent_2_dict.items()):\n",
    "        reorder_sent_2_list.append(value)\n",
    "        \n",
    "\n",
    "    return [torch.from_numpy(np.array(sent1_data_list)).cuda(), \n",
    "            torch.from_numpy(np.array(sent2_data_list)).cuda(), \n",
    "            torch.cuda.LongTensor(sent1_length_list), torch.cuda.LongTensor(sent2_length_list), \n",
    "            torch.cuda.LongTensor(label_list), torch.cuda.LongTensor(indices_list), \n",
    "            torch.cuda.LongTensor(reorder_sent_2_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "def show_result(model_path, hidden_size, l2_penalty, num_epochs = 10, max_vocab_size=50000, batch_size=32, kernel_size=3, \n",
    "                learning_rate=3e-4, phase=\"test\", model_type=\"rnn\", interaction=\"concat\", relu=False):\n",
    "    words_ft, idx2words_ft, ordered_words_ft, loaded_embeddings_ft = set_words_data(max_vocab_size)\n",
    "    \n",
    "    mnil_val_sent1_indices, mnil_val_sent2_indices, mnil_val_label_indices = dict(), dict(), dict()\n",
    "    for genre in mnil_val[\"genre\"].unique():\n",
    "        temp = mnil_val[mnil_val.genre==genre]\n",
    "        mnil_val_sent1_indices[genre] = token2index_dataset(temp['sentence1'], words_ft)\n",
    "        mnil_val_sent2_indices[genre] = token2index_dataset(temp['sentence2'], words_ft)\n",
    "        mnil_val_label_indices[genre] = label2index_dataset(temp['label'], label_to_idx)\n",
    "        \n",
    "    mnil_val_loaders = dict()\n",
    "    for genre in mnil_val[\"genre\"].unique():\n",
    "        temp = SNILDataset(mnil_val_sent1_indices[genre], mnil_val_sent2_indices[genre], mnil_val_label_indices[genre])\n",
    "        mnil_val_loaders[genre] = torch.utils.data.DataLoader(dataset=temp, \n",
    "                                                   batch_size=batch_size,\n",
    "                                                   collate_fn=snil_collate_func,\n",
    "                                                   shuffle=False)\n",
    "        \n",
    "    snil_val_sent1_indices = token2index_dataset(snil_val['sentence1'], words_ft)\n",
    "    snil_val_sent2_indices = token2index_dataset(snil_val['sentence2'], words_ft)\n",
    "    snil_val_label_indices = label2index_dataset(snil_val['label'], label_to_idx)\n",
    "    \n",
    "    val_dataset = SNILDataset(snil_val_sent1_indices, snil_val_sent2_indices, snil_val_label_indices)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=snil_collate_func,\n",
    "                                               shuffle=False)\n",
    "    if model_type==\"rnn\":\n",
    "        model = RNN(None, \n",
    "                    emb_size=300, \n",
    "                    hidden_size=hidden_size, \n",
    "                    num_layers=1, \n",
    "                    num_classes=3, \n",
    "                    vocab_size=len(idx2words_ft),\n",
    "                    interaction=interaction,\n",
    "                    relu=relu\n",
    "                   ).cuda()\n",
    "    elif model_type==\"cnn\":\n",
    "        # num_layers parameter for CNN is not used and ignored.\n",
    "        model = CNN(None, \n",
    "                    emb_size=300, \n",
    "                    hidden_size=hidden_size, \n",
    "                    num_layers=1, \n",
    "                    num_classes=3, \n",
    "                    kernel_size=kernel_size,\n",
    "                    vocab_size=len(idx2words_ft),\n",
    "                    interaction=interaction,\n",
    "                    relu=relu\n",
    "                   ).cuda()\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    total_params = 0\n",
    "    for parameter in model.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            print(parameter.shape, parameter.numel())\n",
    "            total_params += parameter.numel()\n",
    "    print(\"total params\", total_params)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    test_acc, _ = test_model(val_loader, model)\n",
    "    print(\"val acc:\", test_acc)\n",
    "    if phase==\"test\":\n",
    "        for genre in mnil_val[\"genre\"].unique():\n",
    "            test_acc, _ = test_model(mnil_val_loaders[genre], model)\n",
    "            print(genre, \"test acc:\", test_acc)\n",
    "    elif phase in [\"fiction\", \"telephone\", \"slate\", \"government\", \"travel\"]:\n",
    "        experiment_name = \"{}_mnli_train_fixed_{}_epochs_{}_maxvocab_{}_hid_{}_batch_{}_kernel_{}_lr_{}_l2_{}_interaction_{}_relu_{}\".format(\n",
    "            model_type,\n",
    "            phase, \n",
    "            num_epochs,\n",
    "            max_vocab_size,\n",
    "            hidden_size, \n",
    "            batch_size, \n",
    "            kernel_size,\n",
    "            learning_rate,\n",
    "            l2_penalty,\n",
    "            interaction,\n",
    "            relu\n",
    "        )\n",
    "        try:\n",
    "            os.mkdir(experiment_name)\n",
    "        except:\n",
    "            print(\"this experiment is already done.\")\n",
    "            return\n",
    "        \n",
    "        train_temp = mnil_train[mnil_train.genre==phase]\n",
    "        mnil_train_sent1_indices = token2index_dataset(train_temp['sentence1'], words_ft)\n",
    "        mnil_train_sent2_indices = token2index_dataset(train_temp['sentence2'], words_ft)\n",
    "        mnil_train_label_indices = label2index_dataset(train_temp['label'], label_to_idx)\n",
    "\n",
    "        train_temp = SNILDataset(mnil_train_sent1_indices, mnil_train_sent2_indices, mnil_train_label_indices)\n",
    "        mnil_train_loader = torch.utils.data.DataLoader(dataset=train_temp, \n",
    "                                                       batch_size=batch_size,\n",
    "                                                       collate_fn=snil_collate_func,\n",
    "                                                       shuffle=False)\n",
    "        # Criterion and Optimizer\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_penalty)\n",
    "\n",
    "        # Train the model\n",
    "        total_step = len(mnil_train_loader)\n",
    "\n",
    "        loss_lists = []\n",
    "        val_acc_list = []           \n",
    "        val_loss_lists = []           \n",
    "        #print(model.embedding.weight[:,2:].shape)\n",
    "        print(\"fixed, three objects will be saved in three separate files\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            loss_list = []\n",
    "            for i, (data1, data2, lengths1, lengths2, labels, keys, reorder_sent2_list) in enumerate(mnil_train_loader):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                # Forward pass\n",
    "                outputs = model(data1, data2, lengths1, lengths2, reorder_sent2_list)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward and optimize\n",
    "                loss.backward()\n",
    "                #print(model.embedding.weight.grad.data[2:,:].shape)\n",
    "                model.embedding.weight.grad.data[2:,:].fill_(0)\n",
    "                loss_list.append(loss.item())\n",
    "                optimizer.step()\n",
    "                # validate every 100 iterations\n",
    "\n",
    "                if i > 0 and i % 100 == 0:\n",
    "                    # validate\n",
    "                    val_acc, _ = test_model(mnil_val_loaders[genre], model)\n",
    "                    if len(val_acc_list) == 0 or val_acc > max(val_acc_list):\n",
    "                        save_path = \"{}/epoch_{}_step_{}.p\".format(experiment_name, epoch, i)\n",
    "                        torch.save(model.state_dict(), save_path)\n",
    "                        print(\"saved\", save_path)\n",
    "                    val_acc_list.append(val_acc)\n",
    "                    print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                               epoch+1, num_epochs, i+1, len(mnil_train_loader), val_acc))\n",
    "\n",
    "            loss_lists.append(np.mean(np.array(loss_list)))\n",
    "\n",
    "            # validate\n",
    "            val_acc, val_loss_list = test_model(mnil_val_loaders[genre], model)\n",
    "            val_loss_lists.append(np.mean(np.array(val_loss_list)))\n",
    "            if len(val_acc_list) == 0 or val_acc > max(val_acc_list):\n",
    "                save_path = \"{}/epoch_{}_step_epoch_done.p\".format(experiment_name, epoch)\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(\"saved\", save_path)\n",
    "            val_acc_list.append(val_acc)\n",
    "            print('Epoch: [{}/{}], Validation Acc: {}, train mean loss: {}, val mean loss: {}'.format(\n",
    "                        epoch+1, num_epochs, val_acc, loss_lists[-1], val_loss_lists[-1]))\n",
    "\n",
    "\n",
    "            pkl.dump(loss_lists, open(\"{}/loss.p\".format(experiment_name), \"wb\"))\n",
    "            pkl.dump(val_acc_list, open(\"{}/acc.p\".format(experiment_name), \"wb\"))\n",
    "            pkl.dump(val_loss_lists, open(\"{}/val_loss.p\".format(experiment_name), \"wb\"))\n",
    "    elif phase==\"val\":\n",
    "        k=10\n",
    "        val_acc = test_model(val_loader, model)\n",
    "        print(\"val acc:\", val_acc)\n",
    "\n",
    "        model.eval()\n",
    "        correct_examples_list, wrong_examples_list = [], []\n",
    "        for data1, data2, lengths1, lengths2, labels, keys, reorder_sent2_list in val_loader:\n",
    "            if len(correct_examples_list) >= k and len(wrong_examples_list) >= k:\n",
    "                break\n",
    "            outputs = F.softmax(model(data1, data2, lengths1, lengths2, reorder_sent2_list), dim=1)\n",
    "            predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "\n",
    "            correct_list = predicted.eq(labels.view_as(predicted)).flatten().data.cpu().numpy()\n",
    "            labels = labels.flatten().data.cpu().numpy()\n",
    "            prediction = predicted.flatten().data.cpu().numpy()\n",
    "\n",
    "            for i in range(labels.shape[0]):\n",
    "                is_correct = correct_list[i]\n",
    "                if len(correct_examples_list) >= k and len(wrong_examples_list) >= k:\n",
    "                    break\n",
    "                if is_correct:\n",
    "                    correct_examples_list.append( (labels[i], prediction[i], keys[i]) )\n",
    "                else:\n",
    "                    wrong_examples_list.append( (labels[i], prediction[i], keys[i]) )\n",
    "            \n",
    "        print(\"Correct examples:\")\n",
    "\n",
    "        for label, prediction, index in correct_examples_list[:k]:\n",
    "            print(\"true label is\", idx_to_label[label], \", prediction is\", idx_to_label[prediction]) \n",
    "            print(\"original sentense\")       \n",
    "            print(snil_val.iloc[int(index)])\n",
    "        print(\"\\nWrong examples:\")\n",
    "        for label, prediction, index in wrong_examples_list[:k]:\n",
    "            print(\"true label is\", idx_to_label[label], \", prediction is\", idx_to_label[prediction])\n",
    "            print(\"original sentense\")\n",
    "            print(snil_val.iloc[int(index)])    \n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    val_loss_list = []\n",
    "    for data1, data2, lengths1, lengths2, labels, keys, reorder_sent2_list in loader:\n",
    "        #data_batch, lengths_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data1, data2, lengths1, lengths2, reorder_sent2_list), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss_list.append(loss.item())\n",
    "    #print()\n",
    "    return (100 * correct / total), val_loss_list\n",
    "\n",
    "def train(name, hidden_size, max_vocab_size=50000, batch_size=32, kernel_size=3, learning_rate=3e-4, num_epochs=30, \n",
    "          l2_penalty=0, model_type=\"rnn\", shuffle=False, interaction=\"concat\", relu=False):\n",
    "    \n",
    "    #assert n in [1,2,3,4]\n",
    "    \n",
    "    experiment_name = \"{}_snli_{}_maxvocab_{}_hid_{}_batch_{}_kernel_{}_lr_{}_l2_{}_interaction_{}_relu_{}\".format(\n",
    "        model_type,\n",
    "        name, \n",
    "        max_vocab_size,\n",
    "        hidden_size, \n",
    "        batch_size, \n",
    "        kernel_size,\n",
    "        learning_rate,\n",
    "        l2_penalty,\n",
    "        interaction,\n",
    "        relu\n",
    "    )\n",
    "    try:\n",
    "        os.mkdir(experiment_name)\n",
    "    except:\n",
    "        print(\"this experiment is already done.\")\n",
    "        return\n",
    "    \n",
    "    #words_ft, idx2words_ft, ordered_words_ft = set_words_data(max_vocab_size)\n",
    "    words_ft, idx2words_ft, ordered_words_ft, loaded_embeddings_ft = set_words_data(max_vocab_size)\n",
    "    \n",
    "    snil_train_sent1_indices = token2index_dataset(snil_train['sentence1'], words_ft)\n",
    "    snil_train_sent2_indices = token2index_dataset(snil_train['sentence2'], words_ft)\n",
    "    snil_val_sent1_indices = token2index_dataset(snil_val['sentence1'], words_ft)\n",
    "    snil_val_sent2_indices = token2index_dataset(snil_val['sentence2'], words_ft)\n",
    "    snil_train_label_indices = label2index_dataset(snil_train['label'], label_to_idx)\n",
    "    snil_val_label_indices = label2index_dataset(snil_val['label'], label_to_idx)\n",
    "    \n",
    "    train_dataset = SNILDataset(snil_train_sent1_indices, snil_train_sent2_indices, snil_train_label_indices)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=snil_collate_func,\n",
    "                                               shuffle=True)    \n",
    "    val_dataset = SNILDataset(snil_val_sent1_indices, snil_val_sent2_indices, snil_val_label_indices)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=snil_collate_func,\n",
    "                                               shuffle=False)\n",
    "    \n",
    "\n",
    "    if model_type == \"rnn\":\n",
    "        model = RNN(loaded_embeddings_ft, \n",
    "                    emb_size=300, \n",
    "                    hidden_size=hidden_size,\n",
    "                    num_layers=1, \n",
    "                    num_classes=3, \n",
    "                    shuffle=shuffle,\n",
    "                    vocab_size=len(idx2words_ft),\n",
    "                    interaction=interaction,\n",
    "                    relu=relu\n",
    "                   ).cuda()\n",
    "    elif model_type == \"cnn\":\n",
    "        # num_layers parameter for CNN is not used and ignored.\n",
    "        model = CNN(loaded_embeddings_ft, \n",
    "                    emb_size=300, \n",
    "                    hidden_size=hidden_size,\n",
    "                    num_layers=1, \n",
    "                    kernel_size=kernel_size,\n",
    "                    num_classes=3, \n",
    "                    shuffle=shuffle,\n",
    "                    vocab_size=len(idx2words_ft),\n",
    "                    interaction=interaction,\n",
    "                    relu=relu\n",
    "                   ).cuda()\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    total_params = 0\n",
    "    for parameter in model.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            print(parameter.shape, parameter.numel())\n",
    "            total_params += parameter.numel()\n",
    "    print(\"total params\", total_params)\n",
    "    #learning_rate = 3e-4\n",
    "    #num_epochs = 10 # number epoch to train\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_penalty)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    loss_lists = []\n",
    "    val_acc_list = []           \n",
    "    val_loss_lists = []           \n",
    "    #print(model.embedding.weight[:,2:].shape)\n",
    "    print(\"fixed, three objects will be saved in three separate files\")\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_list = []\n",
    "        for i, (data1, data2, lengths1, lengths2, labels, keys, reorder_sent2_list) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data1, data2, lengths1, lengths2, reorder_sent2_list)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            #print(model.embedding.weight.grad.data[2:,:].shape)\n",
    "            model.embedding.weight.grad.data[2:,:].fill_(0)\n",
    "            loss_list.append(loss.item())\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                val_acc, _ = test_model(val_loader, model)\n",
    "                if len(val_acc_list) == 0 or val_acc > max(val_acc_list):\n",
    "                    save_path = \"{}/epoch_{}_step_{}.p\".format(experiment_name, epoch, i)\n",
    "                    torch.save(model.state_dict(), save_path)\n",
    "                    print(\"saved\", save_path)\n",
    "                val_acc_list.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "        loss_lists.append(np.mean(np.array(loss_list)))\n",
    "\n",
    "        # validate\n",
    "        val_acc, val_loss_list = test_model(val_loader, model)\n",
    "        val_loss_lists.append(np.mean(np.array(val_loss_list)))\n",
    "        if len(val_acc_list) == 0 or val_acc > max(val_acc_list):\n",
    "            save_path = \"{}/epoch_{}_step_epoch_done.p\".format(experiment_name, epoch)\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\"saved\", save_path)\n",
    "        val_acc_list.append(val_acc)\n",
    "        print('Epoch: [{}/{}], Validation Acc: {}, train mean loss: {}, val mean loss: {}'.format(\n",
    "                    epoch+1, num_epochs, val_acc, loss_lists[-1], val_loss_lists[-1]))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "        pkl.dump(loss_lists, open(\"{}/loss.p\".format(experiment_name), \"wb\"))\n",
    "        pkl.dump(val_acc_list, open(\"{}/acc.p\".format(experiment_name), \"wb\"))\n",
    "        pkl.dump(val_loss_lists, open(\"{}/val_loss.p\".format(experiment_name), \"wb\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# max seq length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_list_1, len_list_2 = [], []\n",
    "for each in snil_train['sentence1']:\n",
    "    len_list_1.append(len(each))\n",
    "for each in snil_train['sentence2']:\n",
    "    len_list_2.append(len(each))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(len_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(len_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt, lt, eq = 0, 0, 0\n",
    "for each in len_list_1:\n",
    "    if each > 200:\n",
    "        gt+=1\n",
    "    elif each < 200:\n",
    "        lt += 1\n",
    "    else:\n",
    "        eq += 1\n",
    "gt, lt, eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt, lt, eq = 0, 0, 0\n",
    "for each in len_list_2:\n",
    "    if each > 200:\n",
    "        gt+=1\n",
    "    elif each < 200:\n",
    "        lt += 1\n",
    "    else:\n",
    "        eq += 1\n",
    "gt, lt, eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(len_list_1), max(len_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(len_list_1), np.mean(len_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum sequence length of sentence1 is 406, maximum sequence length of sentence2 is 227.\n",
    "\n",
    "However, only 302 data points out of 100000 (0.3%) sentence1 have length greater than 200. Only 2 datapoints out of 100000 sentence2 have length greater than 200. Therefore, we use 200 for maximum sequence length for both sentence1 and sentence2. This is to have reasonable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    words_ft, idx2words_ft, ordered_words_ft, loaded_embeddings_ft = set_words_data(50000)\n",
    "    \n",
    "    #snil_train_sent1_indices = token2index_dataset(snil_train['sentence1'], words_ft)\n",
    "    #snil_train_sent2_indices = token2index_dataset(snil_train['sentence2'], words_ft)\n",
    "    snil_val_sent1_indices = token2index_dataset(snil_val['sentence1'], words_ft)\n",
    "    snil_val_sent2_indices = token2index_dataset(snil_val['sentence2'], words_ft)\n",
    "    #snil_train_label_indices = label2index_dataset(snil_train['label'], label_to_idx)\n",
    "    snil_val_label_indices = label2index_dataset(snil_val['label'], label_to_idx)\n",
    "    val_dataset = SNILDataset(snil_val_sent1_indices, snil_val_sent2_indices, snil_val_label_indices)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=32,\n",
    "                                               collate_fn=snil_collate_func,\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check all batches are correct\n",
    "for i, (data1, data2, lengths1, lengths2, labels, keys, reorder_sent2_list) in enumerate(val_loader):\n",
    "    for index_in_batch in range(len(data1)):\n",
    "        key = keys.cpu().numpy()[index_in_batch]\n",
    "        print(snil_val.iloc[key])\n",
    "        result = \"\"\n",
    "        for token in data1[index_in_batch].cpu().numpy():\n",
    "            if idx2words_ft[token]=='<pad>':\n",
    "                print(result)\n",
    "                break\n",
    "            result += idx2words_ft[token]\n",
    "            result += \" \"\n",
    "        result = \"\"\n",
    "        for token in data2[reorder_sent2_list,:][index_in_batch].cpu().numpy():\n",
    "            if idx2words_ft[token]=='<pad>':\n",
    "                print(result)\n",
    "                break\n",
    "            result += idx2words_ft[token]\n",
    "            result += \" \"\n",
    "        print(idx_to_label[labels[index_in_batch].cpu().item()])\n",
    "        #print(data1[key])\n",
    "        #print(data2[reorder_sent2_list,:][key])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for i, (data1, data2, lengths1, lengths2, labels, keys, reorder_sent2_list) in enumerate(val_loader):\n",
    "    print(keys)\n",
    "    print(reorder_sent2_list)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    model = RNN(loaded_embeddings_ft, \n",
    "                emb_size=300, \n",
    "                hidden_size=8,\n",
    "                num_layers=1, \n",
    "                num_classes=3, \n",
    "                vocab_size=len(idx2words_ft)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for data1, data2, lengths1, lengths2, labels, keys, reorder_sent2_list in val_loader:\n",
    "        #data_batch, lengths_batch, label_batch = data, lengths, labels\n",
    "        outputs = model(data1, data2, lengths1, lengths2, reorder_sent2_list)\n",
    "        print(criterion(outputs, labels).item())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        model = CNN(loaded_embeddings_ft, \n",
    "                    emb_size=300, \n",
    "                    hidden_size=hidden_size,\n",
    "                    num_layers=1, \n",
    "                    kernel_size=kernel_size,\n",
    "                    num_classes=3, \n",
    "                    vocab_size=len(idx2words_ft)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for max_vocab_size in [20000,40000,60000,80000,100000,120000,140000,160000,200000]:\n",
    "    train(\"fixed_kernel_size\", 256, max_vocab_size, 32, model_type=\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_lists, val_acc_lists = dict(), dict()\n",
    "for n in [20000,40000,50000,60000,70000,80000,100000,120000]:\n",
    "    loss_lists[n] = unpickle_from_file('cnn_snli_fixed_kernel_size_maxvocab_{}_hid_256_batch_32_lr_0.0003_l2_0/loss.p'.format(n))\n",
    "    val_acc_lists[n] = unpickle_from_file('cnn_snli_fixed_kernel_size_maxvocab_{}_hid_256_batch_32_lr_0.0003_l2_0/acc.p'.format(n))\n",
    "best_hyperparam, best_val = plot_train_validation(loss_lists, val_acc_lists, \"max vocab size\")\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitely a bug, probably mixing batch order\n",
    "show_result('cnn_snli_fixed_kernel_size_maxvocab_60000_hid_256_batch_32_lr_0.0003_l2_0/epoch_4_step_3000.p', 256, 60000, model_type=\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result('cnn_snli_fixed_kernel_size_maxvocab_70000_hid_256_batch_32_lr_0.0003_l2_0/epoch_2_step_2200.p', 256, 70000, model_type=\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"all_kernel_size_fixed\", 256, 70000, batch_size=32, num_epochs=15, kernel_size=1, model_type=\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kernel_size in [3,5,7,9,11]:\n",
    "    train(\"all_kernel_size_fixed\", 256, 70000, batch_size=32, num_epochs=15, kernel_size=kernel_size, model_type=\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_lists, val_acc_lists = dict(), dict()\n",
    "for n in [1,3,5,7,9,11]:\n",
    "    loss_lists[n] = unpickle_from_file('cnn_snli_all_kernel_size_fixed_maxvocab_70000_hid_256_batch_32_kernel_{}_lr_0.0003_l2_0/loss.p'.format(n))\n",
    "    val_acc_lists[n] = unpickle_from_file('cnn_snli_all_kernel_size_fixed_maxvocab_70000_hid_256_batch_32_kernel_{}_lr_0.0003_l2_0/acc.p'.format(n))\n",
    "best_hyperparam, best_val = plot_train_validation(loss_lists, val_acc_lists, \"kernel size\")\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better to align y axis for two loss graph\n",
    "\n",
    "loss_lists, val_acc_lists, val_loss_lists = dict(), dict(), dict()\n",
    "for n in [1,3,5,7,9,11]:\n",
    "    loss_lists[n] = unpickle_from_file('cnn_snli_all_kernel_size_fixed_maxvocab_70000_hid_256_batch_32_kernel_{}_lr_0.0003_l2_0/loss.p'.format(n))\n",
    "    val_acc_lists[n] = unpickle_from_file('cnn_snli_all_kernel_size_fixed_maxvocab_70000_hid_256_batch_32_kernel_{}_lr_0.0003_l2_0/acc.p'.format(n))\n",
    "    val_loss_lists[n] = unpickle_from_file('cnn_snli_all_kernel_size_fixed_maxvocab_70000_hid_256_batch_32_kernel_{}_lr_0.0003_l2_0/val_loss.p'.format(n))\n",
    "best_hyperparam, best_val = plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, \"kernel size\")\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "for i in [1,3,5,7,9,11]:\n",
    "    model = CNN(None, \n",
    "                emb_size=300, \n",
    "                hidden_size=256,\n",
    "                num_layers=1,\n",
    "                kernel_size=i,\n",
    "                num_classes=3, \n",
    "                vocab_size=70000).cuda()\n",
    "    total_params = 0\n",
    "    for parameter in model.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            print(parameter.shape, parameter.numel())\n",
    "            total_params += parameter.numel()\n",
    "    print(\"kernel size:\", i, \"total params\", total_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,2,4,8,16,32,64,128,512,1024]:\n",
    "    model = CNN(None, \n",
    "                emb_size=300, \n",
    "                hidden_size=i,\n",
    "                num_layers=1,\n",
    "                kernel_size=1,\n",
    "                num_classes=3, \n",
    "                vocab_size=70000).cuda()\n",
    "    total_params = 0\n",
    "    for parameter in model.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            print(parameter.shape, parameter.numel())\n",
    "            total_params += parameter.numel()\n",
    "    print(\"kernel size:\", i, \"total params\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_size in [2,4,8,16,32,64,128,512,1024]:\n",
    "    train(\"all_kernel_size_fixed\", hidden_size, 70000, batch_size=32, num_epochs=30, kernel_size=1, model_type=\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better to align y axis for two loss graph\n",
    "\n",
    "loss_lists, val_acc_lists, val_loss_lists = dict(), dict(), dict()\n",
    "for n in [2,4,8,16,32,64,128,256,512,1024]:\n",
    "    loss_lists[n] = unpickle_from_file('cnn_snli_all_kernel_size_fixed_maxvocab_70000_hid_{}_batch_32_kernel_1_lr_0.0003_l2_0/loss.p'.format(n))\n",
    "    val_acc_lists[n] = unpickle_from_file('cnn_snli_all_kernel_size_fixed_maxvocab_70000_hid_{}_batch_32_kernel_1_lr_0.0003_l2_0/acc.p'.format(n))\n",
    "    val_loss_lists[n] = unpickle_from_file('cnn_snli_all_kernel_size_fixed_maxvocab_70000_hid_{}_batch_32_kernel_1_lr_0.0003_l2_0/val_loss.p'.format(n))\n",
    "best_hyperparam, best_val = plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, \"hidden size\")\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Better to align y axis for two loss graph\n",
    "\n",
    "loss_lists, val_acc_lists, val_loss_lists = dict(), dict(), dict()\n",
    "for n in [8,16,32,64,128,256,512,1024]:\n",
    "    loss_lists[n] = unpickle_from_file('cnn_snli_all_kernel_size_fixed_maxvocab_70000_hid_{}_batch_32_kernel_1_lr_0.0003_l2_0/loss.p'.format(n))\n",
    "    val_acc_lists[n] = unpickle_from_file('cnn_snli_all_kernel_size_fixed_maxvocab_70000_hid_{}_batch_32_kernel_1_lr_0.0003_l2_0/acc.p'.format(n))\n",
    "    val_loss_lists[n] = unpickle_from_file('cnn_snli_all_kernel_size_fixed_maxvocab_70000_hid_{}_batch_32_kernel_1_lr_0.0003_l2_0/val_loss.p'.format(n))\n",
    "best_hyperparam, best_val = plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, \"hidden size\")\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(1e-06,1e-02,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-6,1,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-12,1,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l2 in np.logspace(-12,1,14):\n",
    "    train(\"l2\", 256, 70000, batch_size=32, num_epochs=30, kernel_size=1, l2_penalty=l2, model_type=\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better to align y axis for two loss graph\n",
    "\n",
    "loss_lists, val_acc_lists, val_loss_lists = dict(), dict(), dict()\n",
    "for n in np.logspace(-12,1,14):\n",
    "    loss_lists[n] = unpickle_from_file('cnn_snli_l2_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_{}/loss.p'.format(n))\n",
    "    val_acc_lists[n] = unpickle_from_file('cnn_snli_l2_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_{}/acc.p'.format(n))\n",
    "    val_loss_lists[n] = unpickle_from_file('cnn_snli_l2_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_{}/val_loss.p'.format(n))\n",
    "best_hyperparam, best_val = plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, \"hidden size\", log=True)\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better to align y axis for two loss graph\n",
    "\n",
    "loss_lists, val_acc_lists, val_loss_lists = dict(), dict(), dict()\n",
    "for n in np.logspace(-12,1,14):\n",
    "    loss_lists[n] = unpickle_from_file('cnn_snli_l2_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_{}/loss.p'.format(n))\n",
    "    val_acc_lists[n] = unpickle_from_file('cnn_snli_l2_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_{}/acc.p'.format(n))\n",
    "    val_loss_lists[n] = unpickle_from_file('cnn_snli_l2_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_{}/val_loss.p'.format(n))\n",
    "best_hyperparam, best_val = plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, \"l2\", log=True)\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#verified validation acc stays around 33.1\n",
    "#train(\"test_shuffle\", 256, 70000, batch_size=32, num_epochs=30, kernel_size=1, l2_penalty=1e-09, model_type=\"cnn\", shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try featurewise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"interaction_fixed\", 256, 70000, batch_size=32, num_epochs=30, kernel_size=1, model_type=\"cnn\", interaction='featurewise_multiplication')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Less performance around 67.9\n",
    "# train(\"interaction_fixed\", 256, 70000, batch_size=32, num_epochs=30, kernel_size=3, model_type=\"cnn\", interaction='featurewise_multiplication')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l2 in np.logspace(-12,1,14):\n",
    "    train(\"interaction_l2\", 256, 70000, batch_size=32, num_epochs=30, kernel_size=1, l2_penalty=l2, model_type=\"cnn\", interaction='featurewise_multiplication')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better to align y axis for two loss graph\n",
    "\n",
    "loss_lists, val_acc_lists, val_loss_lists = dict(), dict(), dict()\n",
    "loss_lists['concat'] = unpickle_from_file('cnn_snli_all_kernel_size_fixed_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_0/loss.p')\n",
    "val_acc_lists['concat'] = unpickle_from_file('cnn_snli_all_kernel_size_fixed_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_0/acc.p')\n",
    "val_loss_lists['concat'] = unpickle_from_file('cnn_snli_all_kernel_size_fixed_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_0/val_loss.p')\n",
    "loss_lists['multiply'] = unpickle_from_file('cnn_snli_interaction_fixed_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_0_interaction_featurewise_multiplication/loss.p')\n",
    "val_acc_lists['multiply'] = unpickle_from_file('cnn_snli_interaction_fixed_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_0_interaction_featurewise_multiplication/acc.p')\n",
    "val_loss_lists['multiply'] = unpickle_from_file('cnn_snli_interaction_fixed_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_0_interaction_featurewise_multiplication/val_loss.p')\n",
    "best_hyperparam, best_val = plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, \"combining two sentences\")\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better to align y axis for two loss graph\n",
    "\n",
    "loss_lists, val_acc_lists, val_loss_lists = dict(), dict(), dict()\n",
    "for n in np.logspace(-12,-1,12):\n",
    "    loss_lists[n] = unpickle_from_file('cnn_snli_interaction_l2_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_{}_interaction_featurewise_multiplication/loss.p'.format(n))\n",
    "    val_acc_lists[n] = unpickle_from_file('cnn_snli_interaction_l2_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_{}_interaction_featurewise_multiplication/acc.p'.format(n))\n",
    "    val_loss_lists[n] = unpickle_from_file('cnn_snli_interaction_l2_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_{}_interaction_featurewise_multiplication/val_loss.p'.format(n))\n",
    "best_hyperparam, best_val = plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, \"l2, multiply\", log=True)\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try weight sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result('rnn_snli_ftgru_maxvocab_50000_hid_256_batch_64_lr_0.0003/epoch_8_step_3000.p',256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result('cnn_snli_interaction_l2_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_1e-10_interaction_featurewise_multiplication/epoch_16_step_2500.p',256,max_vocab_size=70000,kernel_size=1,model_type=\"cnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_size in [2,4,8,16,32,64,128,512,1024]:\n",
    "    train(\"step2\", hidden_size, 70000, batch_size=32, num_epochs=30, model_type=\"rnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(\"step2\", 256, 70000, batch_size=32, num_epochs=30, model_type=\"rnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better to align y axis for two loss graph\n",
    "\n",
    "loss_lists, val_acc_lists, val_loss_lists = dict(), dict(), dict()\n",
    "for n in [2,4,8,16,32,64,128,256,512,1024]:\n",
    "    loss_lists[n] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_{}_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication/loss.p'.format(n))\n",
    "    val_acc_lists[n] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_{}_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication/acc.p'.format(n))\n",
    "    val_loss_lists[n] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_{}_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication/val_loss.p'.format(n))\n",
    "best_hyperparam, best_val = plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, \"hidden size\")\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [2,4,8,16,32,64,128,256,512,1024]:\n",
    "    model = RNN(None, \n",
    "                emb_size=300, \n",
    "                hidden_size=i,\n",
    "                num_layers=1,\n",
    "                num_classes=3, \n",
    "                vocab_size=70000,\n",
    "               interaction=\"featurewise_multiplication\")\n",
    "    total_params = 0\n",
    "    for parameter in model.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            print(parameter.shape, parameter.numel())\n",
    "            total_params += parameter.numel()\n",
    "    print(\"kernel size:\", i, \"total params\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better to align y axis for two loss graph\n",
    "\n",
    "loss_lists, val_acc_lists, val_loss_lists = dict(), dict(), dict()\n",
    "for n in np.logspace(-12,-10,3):\n",
    "    loss_lists[n] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_{}_interaction_featurewise_multiplication/loss.p'.format(n))\n",
    "    val_acc_lists[n] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_{}_interaction_featurewise_multiplication/acc.p'.format(n))\n",
    "    val_loss_lists[n] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_{}_interaction_featurewise_multiplication/val_loss.p'.format(n))\n",
    "best_hyperparam, best_val = plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, \"l2 penalty (temp)\")\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-16,-13,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in reversed(np.logspace(-12,-2,11)):\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(\"step2\", 512, 70000, batch_size=32, num_epochs=30, model_type=\"rnn\",interaction=\"concat\")\n",
    "for l2 in np.logspace(-12,1,14):\n",
    "    train(\"step2\", 512, 70000, batch_size=32, num_epochs=30, l2_penalty=l2, model_type=\"rnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l2 in reversed(np.logspace(-16,-13,4)):\n",
    "    train(\"step2\", 512, 70000, batch_size=32, num_epochs=30, l2_penalty=l2, model_type=\"rnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better to align y axis for two loss graph\n",
    "\n",
    "loss_lists, val_acc_lists, val_loss_lists = dict(), dict(), dict()\n",
    "loss_lists['concat'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_concat/loss.p')\n",
    "val_acc_lists['concat'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_concat/acc.p')\n",
    "val_loss_lists['concat'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_concat/val_loss.p')\n",
    "loss_lists['multiply'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication/loss.p')\n",
    "val_acc_lists['multiply'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication/acc.p')\n",
    "val_loss_lists['multiply'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication/val_loss.p')\n",
    "best_hyperparam, best_val = plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, \"combining two sentences\")\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-16,-2,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better to align y axis for two loss graph\n",
    "\n",
    "loss_lists, val_acc_lists, val_loss_lists = dict(), dict(), dict()\n",
    "for n in [0]:\n",
    "    loss_lists[n] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_{}_interaction_featurewise_multiplication/loss.p'.format(n))\n",
    "    val_acc_lists[n] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_{}_interaction_featurewise_multiplication/acc.p'.format(n))\n",
    "    val_loss_lists[n] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_{}_interaction_featurewise_multiplication/val_loss.p'.format(n))\n",
    "for n in np.logspace(-16,-3,14):\n",
    "    loss_lists[n] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_{}_interaction_featurewise_multiplication/loss.p'.format(n))\n",
    "    val_acc_lists[n] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_{}_interaction_featurewise_multiplication/acc.p'.format(n))\n",
    "    val_loss_lists[n] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_{}_interaction_featurewise_multiplication/val_loss.p'.format(n))\n",
    "best_hyperparam, best_val = plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, \"l2 penalty (temp)\", log=True)\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(\"step2\", 512, 70000, batch_size=32, num_epochs=30, model_type=\"rnn\",interaction=\"featurewise_multiplication\",relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"interaction_l2_avail\", 256, 70000, batch_size=32, num_epochs=30, kernel_size=1, l2_penalty=1e-10, \n",
    "      model_type=\"cnn\",interaction=\"featurewise_multiplication\",relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better to align y axis for two loss graph\n",
    "\n",
    "loss_lists, val_acc_lists, val_loss_lists = dict(), dict(), dict()\n",
    "loss_lists['False'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication/loss.p')\n",
    "val_acc_lists['False'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication/acc.p')\n",
    "val_loss_lists['False'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication/val_loss.p')\n",
    "loss_lists['True'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication_relu_True/loss.p')\n",
    "val_acc_lists['True'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication_relu_True/acc.p')\n",
    "val_loss_lists['True'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication_relu_True/val_loss.p')\n",
    "best_hyperparam, best_val = plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, \"rnn, relu between two layers\")\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better to align y axis for two loss graph\n",
    "path1 = 'cnn_snli_interaction_l2_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_1e-10_interaction_featurewise_multiplication'\n",
    "path2 = 'cnn_snli_interaction_l2_avail_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_1e-10_interaction_featurewise_multiplication_relu_True'\n",
    "loss_lists, val_acc_lists, val_loss_lists = dict(), dict(), dict()\n",
    "loss_lists['False'] = unpickle_from_file(path1+'/loss.p')\n",
    "val_acc_lists['False'] = unpickle_from_file(path1+'/acc.p')\n",
    "val_loss_lists['False'] = unpickle_from_file(path1+'/val_loss.p')\n",
    "loss_lists['True'] = unpickle_from_file(path2+'/loss.p')\n",
    "val_acc_lists['True'] = unpickle_from_file(path2+'/acc.p')\n",
    "val_loss_lists['True'] = unpickle_from_file(path2+'/val_loss.p')\n",
    "best_hyperparam, best_val = plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, \"cnn, relu between two layers\")\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better to align y axis for two loss graph\n",
    "\n",
    "loss_lists, val_acc_lists, val_loss_lists = dict(), dict(), dict()\n",
    "loss_lists['False'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication/loss.p')\n",
    "val_acc_lists['False'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication/acc.p')\n",
    "val_loss_lists['False'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication/val_loss.p')\n",
    "loss_lists['True'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication_relu_True/loss.p')\n",
    "val_acc_lists['True'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication_relu_True/acc.p')\n",
    "val_loss_lists['True'] = unpickle_from_file('rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication_relu_True/val_loss.p')\n",
    "best_hyperparam, best_val = plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, \"rnn, relu between two layers\")\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better to align y axis for two loss graph\n",
    "path1 = 'cnn_snli_interaction_l2_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_1e-10_interaction_featurewise_multiplication'\n",
    "path2 = 'cnn_snli_interaction_l2_avail_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_1e-10_interaction_featurewise_multiplication_relu_True'\n",
    "loss_lists, val_acc_lists, val_loss_lists = dict(), dict(), dict()\n",
    "loss_lists['False'] = unpickle_from_file(path1+'/loss.p')\n",
    "val_acc_lists['False'] = unpickle_from_file(path1+'/acc.p')\n",
    "val_loss_lists['False'] = unpickle_from_file(path1+'/val_loss.p')\n",
    "loss_lists['True'] = unpickle_from_file(path2+'/loss.p')\n",
    "val_acc_lists['True'] = unpickle_from_file(path2+'/acc.p')\n",
    "val_loss_lists['True'] = unpickle_from_file(path2+'/val_loss.p')\n",
    "best_hyperparam, best_val = plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, \"cnn, relu between two layers\")\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_cnn_model = 'cnn_snli_interaction_l2_avail_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_1e-10_interaction_featurewise_multiplication_relu_True/epoch_16_step_3100.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(best_cnn_model,256,relu=True,l2_penalty=1e-10,phase=\"fiction\",max_vocab_size=70000,kernel_size=1,model_type=\"cnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(best_cnn_model,256,relu=True,l2_penalty=1e-10,phase=\"telephone\",max_vocab_size=70000,kernel_size=1,model_type=\"cnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(best_cnn_model,256,relu=True,l2_penalty=1e-10,phase=\"slate\",max_vocab_size=70000,kernel_size=1,model_type=\"cnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(best_cnn_model,256,relu=True,l2_penalty=1e-10,phase=\"government\",max_vocab_size=70000,kernel_size=1,model_type=\"cnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(best_cnn_model,256,relu=True,l2_penalty=1e-10,phase=\"travel\",max_vocab_size=70000,kernel_size=1,model_type=\"cnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better to align y axis for two loss graph\n",
    "\n",
    "loss_lists, val_acc_lists, val_loss_lists = dict(), dict(), dict()\n",
    "for n in [\"fiction\", \"telephone\", \"slate\", \"government\", \"travel\"]:\n",
    "    loss_lists[n] = unpickle_from_file('cnn_mnli_train_fixed_{}_epochs_10_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_1e-10_interaction_featurewise_multiplication_relu_True/loss.p'.format(n))\n",
    "    val_acc_lists[n] = unpickle_from_file('cnn_mnli_train_fixed_{}_epochs_10_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_1e-10_interaction_featurewise_multiplication_relu_True/acc.p'.format(n))\n",
    "    val_loss_lists[n] = unpickle_from_file('cnn_mnli_train_fixed_{}_epochs_10_maxvocab_70000_hid_256_batch_32_kernel_1_lr_0.0003_l2_1e-10_interaction_featurewise_multiplication_relu_True/val_loss.p'.format(n))\n",
    "best_hyperparam, best_val = plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, \"finetuning category\")\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_best_model = 'rnn_snli_step2_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication_relu_True/epoch_7_step_2800.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(rnn_best_model,512,relu=True,l2_penalty=0,phase=\"fiction\",max_vocab_size=70000,model_type=\"rnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(rnn_best_model,512,relu=True,l2_penalty=0,phase=\"telephone\",max_vocab_size=70000,model_type=\"rnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(rnn_best_model,512,relu=True,l2_penalty=0,phase=\"slate\",max_vocab_size=70000,model_type=\"rnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(rnn_best_model,512,relu=True,l2_penalty=0,phase=\"government\",max_vocab_size=70000,model_type=\"rnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(rnn_best_model,512,relu=True,l2_penalty=0,phase=\"travel\",max_vocab_size=70000,model_type=\"rnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better to align y axis for two loss graph\n",
    "\n",
    "loss_lists, val_acc_lists, val_loss_lists = dict(), dict(), dict()\n",
    "for n in [\"fiction\", \"telephone\", \"slate\", \"government\", \"travel\"]:\n",
    "    loss_lists[n] = unpickle_from_file('rnn_mnli_train_fixed_{}_epochs_10_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication_relu_True/loss.p'.format(n))\n",
    "    val_acc_lists[n] = unpickle_from_file('rnn_mnli_train_fixed_{}_epochs_10_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication_relu_True/acc.p'.format(n))\n",
    "    val_loss_lists[n] = unpickle_from_file('rnn_mnli_train_fixed_{}_epochs_10_maxvocab_70000_hid_512_batch_32_kernel_3_lr_0.0003_l2_0_interaction_featurewise_multiplication_relu_True/val_loss.p'.format(n))\n",
    "best_hyperparam, best_val = plot_train_validation_new(loss_lists, val_loss_lists, val_acc_lists, \"finetuning category\", bar=False)\n",
    "best_hyperparam, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_result(best_cnn_model,256,relu=True,l2_penalty=1e-10,phase=\"val\",max_vocab_size=70000,kernel_size=1,model_type=\"cnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "show_result(best_cnn_model,256,relu=True,l2_penalty=1e-10,phase=\"val\",max_vocab_size=70000,kernel_size=1,model_type=\"cnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "show_result(rnn_best_model,512,relu=True,l2_penalty=0,phase=\"val\",max_vocab_size=70000,model_type=\"rnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(best_cnn_model,256,relu=True,l2_penalty=1e-10,phase=\"test\",max_vocab_size=70000,kernel_size=1,model_type=\"cnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(rnn_best_model,512,relu=True,l2_penalty=0,phase=\"test\",max_vocab_size=70000,model_type=\"rnn\",interaction=\"featurewise_multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlpclass]",
   "language": "python",
   "name": "conda-env-nlpclass-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
